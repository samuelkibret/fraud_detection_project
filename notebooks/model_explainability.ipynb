{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e992cc7",
   "metadata": {},
   "source": [
    "# Cell 1: Add Project Root to Python Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0512f3d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added 'c:\\Users\\skibret\\Downloads\\KAIM\\Week 8\\Project\\fraud_detection_project' to sys.path.\n"
     ]
    }
   ],
   "source": [
    "# notebooks/model_explainability.ipynb - Cell 1: Add project root to path\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Get the current working directory of the notebook (e.g., 'fraud_detection_project/notebooks/')\n",
    "current_dir = os.getcwd()\n",
    "# Get the parent directory (which is 'fraud_detection_project/')\n",
    "project_root = os.path.dirname(current_dir)\n",
    "\n",
    "# Add the project root to sys.path so Python can find 'src'\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)\n",
    "    print(f\"Added '{project_root}' to sys.path.\")\n",
    "else:\n",
    "    print(f\"'{project_root}' already in sys.path.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0eeb7a5",
   "metadata": {},
   "source": [
    "# Cell 2: Import Statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9e7c67d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\skibret\\Downloads\\KAIM\\Week 8\\Project\\fraud_detection_project\\venv8\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# notebooks/model_explainability.ipynb - Cell 2: Import Statements\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import shap # Import shap library\n",
    "\n",
    "# Import functions from our custom scripts\n",
    "from src.model_training import load_processed_data, split_data, handle_imbalance, evaluate_model\n",
    "from src.model_explainability import explain_model_shap, plot_shap_summary, plot_shap_dependence, plot_shap_force\n",
    "\n",
    "# Import scikit-learn preprocessing and models (needed to load preprocessor and potentially the model)\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline as SklearnPipeline # Alias for sklearn's Pipeline\n",
    "\n",
    "# Configure plot styles\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "plt.rcParams['font.size'] = 12"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c070a0d5",
   "metadata": {},
   "source": [
    "# Cell 3: Load Processed Data and Re-run Preprocessing (for consistency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a8b487d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Loading Processed E-commerce Fraud Data ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-26 21:27:21,040 - INFO - Successfully loaded processed data from c:\\Users\\skibret\\Downloads\\KAIM\\Week 8\\Project\\fraud_detection_project\\data\\processed\\processed_ecommerce_fraud.csv. Shape: (151112, 19)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Loading Processed Credit Card Fraud Data ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-26 21:27:22,790 - INFO - Successfully loaded processed data from c:\\Users\\skibret\\Downloads\\KAIM\\Week 8\\Project\\fraud_detection_project\\data\\processed\\processed_creditcard_fraud.csv. Shape: (283726, 31)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Re-running E-commerce Data Preparation & Preprocessing Pipeline ---\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"['class'] not found in axis\"",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 31\u001b[39m\n\u001b[32m     26\u001b[39m ecommerce_X = ecommerce_df.drop(columns=cols_to_drop_ecommerce, errors=\u001b[33m'\u001b[39m\u001b[33mignore\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     27\u001b[39m ecommerce_y = ecommerce_df[\u001b[33m'\u001b[39m\u001b[33mclass\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m     29\u001b[39m ecommerce_X_train_raw, ecommerce_X_test_raw, ecommerce_y_train, ecommerce_y_test, \\\n\u001b[32m     30\u001b[39m initial_ecommerce_categorical_cols, initial_ecommerce_numerical_cols = \\\n\u001b[32m---> \u001b[39m\u001b[32m31\u001b[39m     \u001b[43msplit_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mecommerce_X\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mclass\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m42\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     33\u001b[39m cols_to_drop_ecommerce_from_X = [\n\u001b[32m     34\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mip_address\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mlower_bound_ip_address\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mupper_bound_ip_address\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mip_address_int\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m     35\u001b[39m     \u001b[33m'\u001b[39m\u001b[33msignup_time\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mpurchase_time\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mtime_diff_prev_transaction\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m     36\u001b[39m ]\n\u001b[32m     37\u001b[39m ecommerce_X_train = ecommerce_X_train_raw.drop(columns=cols_to_drop_ecommerce_from_X, errors=\u001b[33m'\u001b[39m\u001b[33mignore\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\skibret\\Downloads\\KAIM\\Week 8\\Project\\fraud_detection_project\\src\\model_training.py:44\u001b[39m, in \u001b[36msplit_data\u001b[39m\u001b[34m(df, target_column, test_size, random_state)\u001b[39m\n\u001b[32m     41\u001b[39m     logger.error(\u001b[33m\"\u001b[39m\u001b[33mDataFrame is None, cannot split data.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     42\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m44\u001b[39m X = \u001b[43mdf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtarget_column\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     45\u001b[39m y = df[target_column]\n\u001b[32m     47\u001b[39m \u001b[38;5;66;03m# Identify categorical and numerical columns *from this X* for return\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\skibret\\Downloads\\KAIM\\Week 8\\Project\\fraud_detection_project\\venv8\\Lib\\site-packages\\pandas\\core\\frame.py:5588\u001b[39m, in \u001b[36mDataFrame.drop\u001b[39m\u001b[34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[39m\n\u001b[32m   5440\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdrop\u001b[39m(\n\u001b[32m   5441\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   5442\u001b[39m     labels: IndexLabel | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   5449\u001b[39m     errors: IgnoreRaise = \u001b[33m\"\u001b[39m\u001b[33mraise\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   5450\u001b[39m ) -> DataFrame | \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   5451\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   5452\u001b[39m \u001b[33;03m    Drop specified labels from rows or columns.\u001b[39;00m\n\u001b[32m   5453\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   5586\u001b[39m \u001b[33;03m            weight  1.0     0.8\u001b[39;00m\n\u001b[32m   5587\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m5588\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   5589\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5590\u001b[39m \u001b[43m        \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5591\u001b[39m \u001b[43m        \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m=\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5592\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5593\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5594\u001b[39m \u001b[43m        \u001b[49m\u001b[43minplace\u001b[49m\u001b[43m=\u001b[49m\u001b[43minplace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5595\u001b[39m \u001b[43m        \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5596\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\skibret\\Downloads\\KAIM\\Week 8\\Project\\fraud_detection_project\\venv8\\Lib\\site-packages\\pandas\\core\\generic.py:4807\u001b[39m, in \u001b[36mNDFrame.drop\u001b[39m\u001b[34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[39m\n\u001b[32m   4805\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m axis, labels \u001b[38;5;129;01min\u001b[39;00m axes.items():\n\u001b[32m   4806\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m4807\u001b[39m         obj = \u001b[43mobj\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_drop_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4809\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m inplace:\n\u001b[32m   4810\u001b[39m     \u001b[38;5;28mself\u001b[39m._update_inplace(obj)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\skibret\\Downloads\\KAIM\\Week 8\\Project\\fraud_detection_project\\venv8\\Lib\\site-packages\\pandas\\core\\generic.py:4849\u001b[39m, in \u001b[36mNDFrame._drop_axis\u001b[39m\u001b[34m(self, labels, axis, level, errors, only_slice)\u001b[39m\n\u001b[32m   4847\u001b[39m         new_axis = axis.drop(labels, level=level, errors=errors)\n\u001b[32m   4848\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m4849\u001b[39m         new_axis = \u001b[43maxis\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4850\u001b[39m     indexer = axis.get_indexer(new_axis)\n\u001b[32m   4852\u001b[39m \u001b[38;5;66;03m# Case for non-unique axis\u001b[39;00m\n\u001b[32m   4853\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\skibret\\Downloads\\KAIM\\Week 8\\Project\\fraud_detection_project\\venv8\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:7136\u001b[39m, in \u001b[36mIndex.drop\u001b[39m\u001b[34m(self, labels, errors)\u001b[39m\n\u001b[32m   7134\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m mask.any():\n\u001b[32m   7135\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m errors != \u001b[33m\"\u001b[39m\u001b[33mignore\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m-> \u001b[39m\u001b[32m7136\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlabels[mask].tolist()\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m not found in axis\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   7137\u001b[39m     indexer = indexer[~mask]\n\u001b[32m   7138\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.delete(indexer)\n",
      "\u001b[31mKeyError\u001b[39m: \"['class'] not found in axis\""
     ]
    }
   ],
   "source": [
    "# notebooks/model_explainability.ipynb - Cell 3: Load Data & Re-run Preprocessing\n",
    "\n",
    "# Ensure necessary imports are at the top of this cell or in Cell 2\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline as SklearnPipeline # Alias for sklearn's Pipeline\n",
    "\n",
    "# Import functions from src/model_training (assuming Cell 1 has added project_root to sys.path)\n",
    "from src.model_training import load_processed_data, split_data, handle_imbalance\n",
    "\n",
    "# Define paths to your processed data, using project_root for absolute paths\n",
    "ecommerce_filepath = os.path.join(project_root, 'data', 'processed', 'processed_ecommerce_fraud.csv')\n",
    "creditcard_filepath = os.path.join(project_root, 'data', 'processed', 'processed_creditcard_fraud.csv')\n",
    "\n",
    "print(\"--- Loading Processed E-commerce Fraud Data ---\")\n",
    "ecommerce_df = load_processed_data(ecommerce_filepath)\n",
    "\n",
    "print(\"\\n--- Loading Processed Credit Card Fraud Data ---\")\n",
    "creditcard_df = load_processed_data(creditcard_filepath)\n",
    "\n",
    "# --- Re-run E-commerce Data Preparation & Preprocessing Pipeline ---\n",
    "ecommerce_X_train_processed = None\n",
    "ecommerce_X_test_processed = None\n",
    "ecommerce_y_train = None\n",
    "ecommerce_y_test = None\n",
    "ecommerce_preprocessor_fitted = None # Will be fitted below\n",
    "ecommerce_feature_names = [] # Initialize list for feature names\n",
    "\n",
    "if ecommerce_df is not None:\n",
    "    print(\"\\n--- Re-running E-commerce Data Preparation & Preprocessing Pipeline ---\")\n",
    "\n",
    "    # Pass the original full dataframe (ecommerce_df) to split_data.\n",
    "    # split_data will handle dropping the 'class' column internally to create X.\n",
    "    ecommerce_X_train_raw, ecommerce_X_test_raw, ecommerce_y_train, ecommerce_y_test, \\\n",
    "    _, _ = \\\n",
    "        split_data(ecommerce_df, target_column='class', test_size=0.3, random_state=42)\n",
    "\n",
    "    # Now, from the X_train_raw and X_test_raw returned by split_data,\n",
    "    # drop the *other* non-feature columns that are not relevant for modeling.\n",
    "    cols_to_drop_ecommerce_from_X_after_split = [\n",
    "        'ip_address', 'lower_bound_ip_address', 'upper_bound_ip_address', 'ip_address_int',\n",
    "        'signup_time', 'purchase_time', 'time_diff_prev_transaction'\n",
    "    ]\n",
    "    ecommerce_X_train = ecommerce_X_train_raw.drop(columns=cols_to_drop_ecommerce_from_X_after_split, errors='ignore')\n",
    "    ecommerce_X_test = ecommerce_X_test_raw.drop(columns=cols_to_drop_ecommerce_from_X_after_split, errors='ignore')\n",
    "\n",
    "    # Re-identify numerical and categorical columns *after* dropping the non-feature columns.\n",
    "    # This ensures the preprocessor uses the correct, final column lists.\n",
    "    ecommerce_numerical_cols_for_preprocessor = ecommerce_X_train.select_dtypes(include=np.number).columns.tolist()\n",
    "    ecommerce_categorical_cols_for_preprocessor = ecommerce_X_train.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "\n",
    "    print(f\"E-commerce Numerical Columns for Preprocessor (after feature selection): {ecommerce_numerical_cols_for_preprocessor}\")\n",
    "    print(f\"E-commerce Categorical Columns for Preprocessor (after feature selection): {ecommerce_categorical_cols_for_preprocessor}\")\n",
    "\n",
    "    # Create and fit the preprocessing pipeline using ColumnTransformer\n",
    "    ecommerce_preprocessor_fitted = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', SklearnPipeline(steps=[\n",
    "                ('imputer', SimpleImputer(strategy='median')), # Impute NaNs in numerical columns\n",
    "                ('scaler', StandardScaler())                    # Scale numerical features\n",
    "            ]), ecommerce_numerical_cols_for_preprocessor),\n",
    "            ('cat', OneHotEncoder(handle_unknown='ignore'), ecommerce_categorical_cols_for_preprocessor) # One-hot encode categorical features\n",
    "        ],\n",
    "        remainder='passthrough' # Keep any other columns not specified (e.g., if you added more later)\n",
    "    ).fit(ecommerce_X_train) # Fit the preprocessor ONLY on the training data to prevent data leakage\n",
    "\n",
    "    # Transform both training and testing sets using the fitted preprocessor\n",
    "    ecommerce_X_train_processed = ecommerce_preprocessor_fitted.transform(ecommerce_X_train)\n",
    "    ecommerce_X_test_processed = ecommerce_preprocessor_fitted.transform(ecommerce_X_test)\n",
    "\n",
    "    # Get feature names after one-hot encoding for plotting with SHAP\n",
    "    ecommerce_feature_names = ecommerce_preprocessor_fitted.get_feature_names_out()\n",
    "    print(f\"Total E-commerce features after preprocessing: {len(ecommerce_feature_names)}\")\n",
    "    print(f\"E-commerce X_train_processed shape: {ecommerce_X_train_processed.shape}\")\n",
    "    print(f\"E-commerce X_test_processed shape: {ecommerce_X_test_processed.shape}\")\n",
    "else:\n",
    "    print(\"E-commerce data not loaded. Skipping preprocessing.\")\n",
    "\n",
    "# --- Re-run Credit Card Data Preparation & Preprocessing Pipeline ---\n",
    "creditcard_X_train_processed = None\n",
    "creditcard_X_test_processed = None\n",
    "creditcard_y_train = None\n",
    "creditcard_y_test = None\n",
    "creditcard_preprocessor_fitted = None # Will be fitted below\n",
    "creditcard_feature_names = [] # Initialize list for feature names\n",
    "\n",
    "if creditcard_df is not None:\n",
    "    print(\"\\n--- Re-running Credit Card Data Preparation & Preprocessing Pipeline ---\")\n",
    "    # Pass the original full dataframe (creditcard_df) to split_data.\n",
    "    # split_data will handle dropping the 'Class' column internally to create X.\n",
    "    creditcard_X_train_raw, creditcard_X_test_raw, creditcard_y_train, creditcard_y_test, \\\n",
    "    _, _ = \\\n",
    "        split_data(creditcard_df, target_column='Class', test_size=0.3, random_state=42)\n",
    "\n",
    "    # For credit card data, typically no extra columns to drop from X_train/X_test\n",
    "    # as V-features, Time, Amount are all relevant.\n",
    "    # So, creditcard_X_train and creditcard_X_test are already clean for preprocessing here.\n",
    "    creditcard_X_train = creditcard_X_train_raw.copy()\n",
    "    creditcard_X_test = creditcard_X_test_raw.copy()\n",
    "\n",
    "\n",
    "    creditcard_numerical_cols_for_preprocessor = creditcard_X_train.select_dtypes(include=np.number).columns.tolist()\n",
    "    creditcard_categorical_cols_for_preprocessor = creditcard_X_train.select_dtypes(include=['object', 'category']).columns.tolist() # Should be empty\n",
    "\n",
    "    print(f\"Credit Card Numerical Columns for Preprocessor (after feature selection): {creditcard_numerical_cols_for_preprocessor}\")\n",
    "    print(f\"Credit Card Categorical Columns for Preprocessor (after feature selection): {creditcard_categorical_cols_for_preprocessor}\")\n",
    "\n",
    "    # Create and fit the preprocessing pipeline using ColumnTransformer\n",
    "    # Only numerical features are expected for credit card data, so only a numerical pipeline\n",
    "    creditcard_preprocessor_fitted = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', SklearnPipeline(steps=[\n",
    "                ('imputer', SimpleImputer(strategy='median')), # Impute NaNs in numerical columns\n",
    "                ('scaler', StandardScaler())                    # Scale numerical features\n",
    "            ]), creditcard_numerical_cols_for_preprocessor)\n",
    "            # No 'cat' transformer needed if no categorical columns are present\n",
    "        ],\n",
    "        remainder='passthrough'\n",
    "    ).fit(creditcard_X_train) # Fit the preprocessor ONLY on the training data\n",
    "\n",
    "    # Transform both training and testing sets\n",
    "    creditcard_X_train_processed = creditcard_preprocessor_fitted.transform(creditcard_X_train)\n",
    "    creditcard_X_test_processed = creditcard_preprocessor_fitted.transform(creditcard_X_test)\n",
    "\n",
    "    # Get feature names after preprocessing for plotting with SHAP\n",
    "    creditcard_feature_names = creditcard_preprocessor_fitted.get_feature_names_out()\n",
    "    print(f\"Total Credit Card features after preprocessing: {len(creditcard_feature_names)}\")\n",
    "    print(f\"Credit Card X_train_processed shape: {creditcard_X_train_processed.shape}\")\n",
    "    print(f\"Credit Card X_test_processed shape: {creditcard_X_test_processed.shape}\")\n",
    "else:\n",
    "    print(\"Credit Card data not loaded. Skipping preprocessing.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41ed12a3",
   "metadata": {},
   "source": [
    "# Cell 4: Identify Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e652b19a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# notebooks/model_explainability.ipynb - Cell 4: Identify and Load Best Model\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier # Import the specific best model\n",
    "\n",
    "# --- For E-commerce Fraud Detection ---\n",
    "best_ecommerce_model_name = \"Random Forest\"\n",
    "# Re-initialize the Random Forest model with the same default parameters used in training\n",
    "# (n_estimators=100, random_state=42, n_jobs=-1)\n",
    "best_ecommerce_model = RandomForestClassifier(random_state=42, n_estimators=100, n_jobs=-1)\n",
    "print(f\"Selected best E-commerce model: {best_ecommerce_model_name}\")\n",
    "\n",
    "# --- For Credit Card Fraud Detection ---\n",
    "best_creditcard_model_name = \"Random Forest\"\n",
    "# Re-initialize the Random Forest model with the same default parameters used in training\n",
    "# (n_estimators=100, random_state=42, n_jobs=-1)\n",
    "best_creditcard_model = RandomForestClassifier(random_state=42, n_estimators=100, n_jobs=-1)\n",
    "print(f\"Selected best Credit Card model: {best_creditcard_model_name}\")\n",
    "\n",
    "# Note: In a real scenario, you would likely save and load your best model\n",
    "# using joblib or pickle to avoid re-training here.\n",
    "# import joblib\n",
    "# best_ecommerce_model = joblib.load('path/to/best_ecommerce_model.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f946a636",
   "metadata": {},
   "source": [
    "# Cell 5: Imbalance Handling (for fitting best model in this notebook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4911c244",
   "metadata": {},
   "outputs": [],
   "source": [
    "# notebooks/model_explainability.ipynb - Cell 5: Imbalance Handling (for fitting best model)\n",
    "\n",
    "ecommerce_X_train_resampled_explain = None\n",
    "ecommerce_y_train_resampled_explain = None\n",
    "creditcard_X_train_resampled_explain = None\n",
    "creditcard_y_train_resampled_explain = None\n",
    "\n",
    "if ecommerce_X_train_processed is not None and ecommerce_y_train is not None:\n",
    "    print(\"\\n--- Handling Imbalance for E-commerce Training Data (SMOTE) for Explainability ---\")\n",
    "    ecommerce_X_train_resampled_explain, ecommerce_y_train_resampled_explain = \\\n",
    "        handle_imbalance(ecommerce_X_train_processed, ecommerce_y_train, strategy='SMOTE', random_state=42)\n",
    "else:\n",
    "    print(\"E-commerce processed training data not available for imbalance handling for explainability.\")\n",
    "\n",
    "if creditcard_X_train_processed is not None and creditcard_y_train is not None:\n",
    "    print(\"\\n--- Handling Imbalance for Credit Card Training Data (SMOTE_and_Undersample) for Explainability ---\")\n",
    "    creditcard_X_train_resampled_explain, creditcard_y_train_resampled_explain = \\\n",
    "        handle_imbalance(creditcard_X_train_processed, creditcard_y_train, strategy='SMOTE_and_Undersample', random_state=42)\n",
    "else:\n",
    "    print(\"Credit Card processed training data not available for imbalance handling for explainability.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ade1805",
   "metadata": {},
   "source": [
    "# Cell 6: Fit Best Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f758871",
   "metadata": {},
   "outputs": [],
   "source": [
    "# notebooks/model_explainability.ipynb - Cell 6: Fit Best Models\n",
    "\n",
    "if best_ecommerce_model is not None and ecommerce_X_train_resampled_explain is not None:\n",
    "    print(f\"\\n--- Fitting {best_ecommerce_model_name} for E-commerce Explainability ---\")\n",
    "    best_ecommerce_model.fit(ecommerce_X_train_resampled_explain, ecommerce_y_train_resampled_explain)\n",
    "    print(f\"{best_ecommerce_model_name} for E-commerce fitted.\")\n",
    "else:\n",
    "    print(\"E-commerce best model or resampled data not available for fitting.\")\n",
    "\n",
    "if best_creditcard_model is not None and creditcard_X_train_resampled_explain is not None:\n",
    "    print(f\"\\n--- Fitting {best_creditcard_model_name} for Credit Card Explainability ---\")\n",
    "    best_creditcard_model.fit(creditcard_X_train_resampled_explain, creditcard_y_train_resampled_explain)\n",
    "    print(f\"{best_creditcard_model_name} for Credit Card fitted.\")\n",
    "else:\n",
    "    print(\"Credit Card best model or resampled data not available for fitting.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc875cf9",
   "metadata": {},
   "source": [
    "# Cell 7: SHAP Explanations for E-commerce Fraud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6342326a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# notebooks/model_explainability.ipynb - Cell 7: SHAP Explanations for E-commerce Fraud\n",
    "\n",
    "ecommerce_explainer = None\n",
    "ecommerce_shap_values = None\n",
    "\n",
    "if best_ecommerce_model is not None and ecommerce_X_test_processed is not None and ecommerce_feature_names is not None:\n",
    "    print(\"\\n--- Generating SHAP Explanations for E-commerce Model ---\")\n",
    "    # For KernelExplainer (e.g. for MLP), you might need a background dataset from X_train_processed\n",
    "    # For TreeExplainer, X_data can be X_test_processed directly\n",
    "    ecommerce_explainer, ecommerce_shap_values = explain_model_shap(\n",
    "        best_ecommerce_model,\n",
    "        ecommerce_X_test_processed, # Use test set for explanation\n",
    "        feature_names=ecommerce_feature_names\n",
    "    )\n",
    "\n",
    "    if ecommerce_explainer is not None and ecommerce_shap_values is not None:\n",
    "        print(\"\\n--- E-commerce SHAP Summary Plot ---\")\n",
    "        plot_shap_summary(\n",
    "            ecommerce_shap_values,\n",
    "            ecommerce_feature_names,\n",
    "            title=f\"SHAP Summary Plot for E-commerce Fraud ({best_ecommerce_model_name})\"\n",
    "        )\n",
    "    else:\n",
    "        print(\"Failed to generate E-commerce SHAP values.\")\n",
    "else:\n",
    "    print(\"E-commerce best model, test data, or feature names not available for SHAP.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "058fcb29",
   "metadata": {},
   "source": [
    "# Cell 8: SHAP Explanations for E-commerce Fraud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "427fd953",
   "metadata": {},
   "outputs": [],
   "source": [
    "# notebooks/model_explainability.ipynb - Cell 8: SHAP Force Plot for E-commerce Fraud (Example)\n",
    "\n",
    "if ecommerce_explainer is not None and ecommerce_shap_values is not None and ecommerce_X_test_processed is not None:\n",
    "    print(\"\\n--- E-commerce SHAP Force Plot (Example for a single prediction) ---\")\n",
    "    # Find an example of a fraudulent transaction in the test set (if any)\n",
    "    fraud_indices = ecommerce_y_test[ecommerce_y_test == 1].index\n",
    "    if not fraud_indices.empty:\n",
    "        example_index = fraud_indices[0] # Take the first fraudulent example\n",
    "        # Need to get the position of this index within the X_test_processed array\n",
    "        # This is tricky because X_test_processed is a NumPy array/sparse matrix\n",
    "        # and doesn't retain original DataFrame indices.\n",
    "        # A simpler approach for demonstration is to pick a row directly from the processed test set:\n",
    "        row_to_explain_idx = 0 # Example: first row in the processed test set\n",
    "        print(f\"Displaying force plot for row {row_to_explain_idx} (original index might differ).\")\n",
    "\n",
    "        plot_shap_force(\n",
    "            ecommerce_explainer,\n",
    "            ecommerce_shap_values,\n",
    "            ecommerce_X_test_processed,\n",
    "            ecommerce_feature_names,\n",
    "            row_index=row_to_explain_idx, # Use the index within the processed array\n",
    "            title=f\"SHAP Force Plot for E-commerce Fraud ({best_ecommerce_model_name})\"\n",
    "        )\n",
    "    else:\n",
    "        print(\"No fraudulent transactions found in E-commerce test set to plot a force plot example.\")\n",
    "else:\n",
    "    print(\"E-commerce SHAP explainer, values, or test data not available for force plot.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13121e60",
   "metadata": {},
   "source": [
    "# Cell 9: SHAP Explanations for Credit Card Fraud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f833e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# notebooks/model_explainability.ipynb - Cell 9: SHAP Explanations for Credit Card Fraud\n",
    "\n",
    "creditcard_explainer = None\n",
    "creditcard_shap_values = None\n",
    "\n",
    "if best_creditcard_model is not None and creditcard_X_test_processed is not None and creditcard_feature_names is not None:\n",
    "    print(\"\\n--- Generating SHAP Explanations for Credit Card Model ---\")\n",
    "    creditcard_explainer, creditcard_shap_values = explain_model_shap(\n",
    "        best_creditcard_model,\n",
    "        creditcard_X_test_processed, # Use test set for explanation\n",
    "        feature_names=creditcard_feature_names\n",
    "    )\n",
    "\n",
    "    if creditcard_explainer is not None and creditcard_shap_values is not None:\n",
    "        print(\"\\n--- Credit Card SHAP Summary Plot ---\")\n",
    "        plot_shap_summary(\n",
    "            creditcard_shap_values,\n",
    "            creditcard_feature_names,\n",
    "            title=f\"SHAP Summary Plot for Credit Card Fraud ({best_creditcard_model_name})\"\n",
    "        )\n",
    "    else:\n",
    "        print(\"Failed to generate Credit Card SHAP values.\")\n",
    "else:\n",
    "    print(\"Credit Card best model, test data, or feature names not available for SHAP.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65f5b0c1",
   "metadata": {},
   "source": [
    "# Cell 10: SHAP Explanations for Credit Card Fraud "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8efd560",
   "metadata": {},
   "outputs": [],
   "source": [
    "# notebooks/model_explainability.ipynb - Cell 10: SHAP Force Plot for Credit Card Fraud (Example)\n",
    "\n",
    "if creditcard_explainer is not None and creditcard_shap_values is not None and creditcard_X_test_processed is not None:\n",
    "    print(\"\\n--- Credit Card SHAP Force Plot (Example for a single prediction) ---\")\n",
    "    fraud_indices = creditcard_y_test[creditcard_y_test == 1].index\n",
    "    if not fraud_indices.empty:\n",
    "        example_index = fraud_indices[0] # Take the first fraudulent example\n",
    "        row_to_explain_idx = 0 # Example: first row in the processed test set\n",
    "        print(f\"Displaying force plot for row {row_to_explain_idx} (original index might differ).\")\n",
    "\n",
    "        plot_shap_force(\n",
    "            creditcard_explainer,\n",
    "            creditcard_shap_values,\n",
    "            creditcard_X_test_processed,\n",
    "            creditcard_feature_names,\n",
    "            row_index=row_to_explain_idx,\n",
    "            title=f\"SHAP Force Plot for Credit Card Fraud ({best_creditcard_model_name})\"\n",
    "        )\n",
    "    else:\n",
    "        print(\"No fraudulent transactions found in Credit Card test set to plot a force plot example.\")\n",
    "else:\n",
    "    print(\"Credit Card SHAP explainer, values, or test data not available for force plot.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv8",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
