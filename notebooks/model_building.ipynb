{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b78024a9",
   "metadata": {},
   "source": [
    "# Cell 1: Add Project Root to Python Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ede843dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added 'c:\\Users\\skibret\\Downloads\\KAIM\\Week 8\\Project\\fraud_detection_project' to sys.path.\n"
     ]
    }
   ],
   "source": [
    "# notebooks/model_building.ipynb - Cell 1: Add project root to path\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Get the current working directory of the notebook (e.g., 'fraud_detection_project/notebooks/')\n",
    "current_dir = os.getcwd()\n",
    "# Get the parent directory (which is 'fraud_detection_project/')\n",
    "project_root = os.path.dirname(current_dir)\n",
    "\n",
    "# Add the project root to sys.path so Python can find 'src'\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)\n",
    "    print(f\"Added '{project_root}' to sys.path.\")\n",
    "else:\n",
    "    print(f\"'{project_root}' already in sys.path.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d232c362",
   "metadata": {},
   "source": [
    "# Cell 2: Import Statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "44fc9720",
   "metadata": {},
   "outputs": [],
   "source": [
    "# notebooks/model_building.ipynb - Cell 2: Import Statements\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline as SklearnPipeline\n",
    "\n",
    "\n",
    "# Import functions from our model_training.py script\n",
    "from src.model_training import (\n",
    "    load_processed_data,\n",
    "    split_data,\n",
    "    handle_imbalance,\n",
    "    evaluate_model,\n",
    "    train_logistic_regression, # NEW\n",
    "    train_decision_tree,       # NEW\n",
    "    train_random_forest,       # NEW\n",
    "    train_gradient_boosting   # NEW\n",
    "\n",
    ")\n",
    "\n",
    "# Import necessary models from scikit-learn (will add more as we go)\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# For CNN/RNN, we'll likely use TensorFlow/Keras or PyTorch, which will be installed later.\n",
    "# from tensorflow.keras.models import Sequential\n",
    "# from tensorflow.keras.layers import Dense, Dropout, Conv1D, MaxPooling1D, Flatten, LSTM\n",
    "\n",
    "# Configure plot styles for better visualization\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "plt.rcParams['font.size'] = 12"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cadff60",
   "metadata": {},
   "source": [
    "# Cell 3: Load Processed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b8f04cd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Loading Processed E-commerce Fraud Data ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-26 20:19:05,940 - INFO - Successfully loaded processed data from c:\\Users\\skibret\\Downloads\\KAIM\\Week 8\\Project\\fraud_detection_project\\data\\processed\\processed_ecommerce_fraud.csv. Shape: (151112, 19)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Loading Processed Credit Card Fraud Data ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-26 20:19:07,330 - INFO - Successfully loaded processed data from c:\\Users\\skibret\\Downloads\\KAIM\\Week 8\\Project\\fraud_detection_project\\data\\processed\\processed_creditcard_fraud.csv. Shape: (283726, 31)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "E-commerce data loaded. Shape: (151112, 19)\n",
      "E-commerce Data Head:\n",
      "   user_id          signup_time        purchase_time  purchase_value  \\\n",
      "0        2  2015-01-11 03:47:13  2015-02-21 10:03:37              54   \n",
      "1        4  2015-06-02 16:40:57  2015-09-26 21:32:16              41   \n",
      "2        8  2015-05-28 07:53:06  2015-08-13 11:53:07              47   \n",
      "3        9  2015-05-16 15:58:32  2015-05-20 23:06:42              62   \n",
      "4       12  2015-01-10 06:25:12  2015-03-04 20:56:37              35   \n",
      "\n",
      "       device_id  source  browser sex  age    ip_address  class  \\\n",
      "0  FGBQNDNBETFJJ     SEO   Chrome   F   25  8.802175e+08      0   \n",
      "1  MKFUIVOHLJBYN  Direct   Safari   F   38  2.785906e+09      0   \n",
      "2  SCQGQALXBUQZJ     SEO   Chrome   M   25  3.560567e+08      0   \n",
      "3  IEZOHXPZBIRTE     SEO  FireFox   M   21  7.591047e+08      0   \n",
      "4  MSNWCFEHKTIOY     Ads   Safari   M   19  2.985180e+09      0   \n",
      "\n",
      "   ip_address_int  country  hour_of_day  day_of_week  time_since_signup  \\\n",
      "0             NaN      NaN           10            5          41.261389   \n",
      "1             NaN      NaN           21            5         116.202303   \n",
      "2             NaN      NaN           11            3          77.166678   \n",
      "3             NaN      NaN           23            2           4.297338   \n",
      "4             NaN      NaN           20            2          53.605150   \n",
      "\n",
      "   time_diff_prev_transaction  transactions_last_7d  purchase_value_last_7d  \n",
      "0                         NaN                   1.0                    54.0  \n",
      "1                         NaN                   1.0                    41.0  \n",
      "2                         NaN                   1.0                    47.0  \n",
      "3                         NaN                   1.0                    62.0  \n",
      "4                         NaN                   1.0                    35.0  \n",
      "\n",
      "Credit Card data loaded. Shape: (283726, 31)\n",
      "Credit Card Data Head:\n",
      "   Time        V1        V2        V3        V4        V5        V6        V7  \\\n",
      "0   0.0 -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388  0.239599   \n",
      "1   0.0  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361 -0.078803   \n",
      "2   1.0 -1.358354 -1.340163  1.773209  0.379780 -0.503198  1.800499  0.791461   \n",
      "3   1.0 -0.966272 -0.185226  1.792993 -0.863291 -0.010309  1.247203  0.237609   \n",
      "4   2.0 -1.158233  0.877737  1.548718  0.403034 -0.407193  0.095921  0.592941   \n",
      "\n",
      "         V8        V9  ...       V21       V22       V23       V24       V25  \\\n",
      "0  0.098698  0.363787  ... -0.018307  0.277838 -0.110474  0.066928  0.128539   \n",
      "1  0.085102 -0.255425  ... -0.225775 -0.638672  0.101288 -0.339846  0.167170   \n",
      "2  0.247676 -1.514654  ...  0.247998  0.771679  0.909412 -0.689281 -0.327642   \n",
      "3  0.377436 -1.387024  ... -0.108300  0.005274 -0.190321 -1.175575  0.647376   \n",
      "4 -0.270533  0.817739  ... -0.009431  0.798278 -0.137458  0.141267 -0.206010   \n",
      "\n",
      "        V26       V27       V28  Amount  Class  \n",
      "0 -0.189115  0.133558 -0.021053  149.62      0  \n",
      "1  0.125895 -0.008983  0.014724    2.69      0  \n",
      "2 -0.139097 -0.055353 -0.059752  378.66      0  \n",
      "3 -0.221929  0.062723  0.061458  123.50      0  \n",
      "4  0.502292  0.219422  0.215153   69.99      0  \n",
      "\n",
      "[5 rows x 31 columns]\n"
     ]
    }
   ],
   "source": [
    "# notebooks/model_building.ipynb - Cell 3: Load Processed Data\n",
    "\n",
    "# Define paths to your processed data, using project_root for absolute paths\n",
    "ecommerce_filepath = os.path.join(project_root, 'data', 'processed', 'processed_ecommerce_fraud.csv')\n",
    "creditcard_filepath = os.path.join(project_root, 'data', 'processed', 'processed_creditcard_fraud.csv')\n",
    "\n",
    "print(\"--- Loading Processed E-commerce Fraud Data ---\")\n",
    "ecommerce_df = load_processed_data(ecommerce_filepath)\n",
    "\n",
    "print(\"\\n--- Loading Processed Credit Card Fraud Data ---\")\n",
    "creditcard_df = load_processed_data(creditcard_filepath)\n",
    "\n",
    "if ecommerce_df is not None:\n",
    "    print(f\"\\nE-commerce data loaded. Shape: {ecommerce_df.shape}\")\n",
    "    print(\"E-commerce Data Head:\")\n",
    "    print(ecommerce_df.head())\n",
    "else:\n",
    "    print(\"\\nE-commerce data not loaded.\")\n",
    "\n",
    "if creditcard_df is not None:\n",
    "    print(f\"\\nCredit Card data loaded. Shape: {creditcard_df.shape}\")\n",
    "    print(\"Credit Card Data Head:\")\n",
    "    print(creditcard_df.head())\n",
    "else:\n",
    "    print(\"\\nCredit Card data not loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad0c8462",
   "metadata": {},
   "source": [
    "# Cell 4: Data Preprocessing for Modeling (One-Hot Encoding, Scaling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dc9f4e7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Preparing E-commerce Data for Modeling ---\n",
      "E-commerce X shape (before split): (151112, 13)\n",
      "E-commerce Numerical Columns for Preprocessor: ['user_id', 'purchase_value', 'age', 'country', 'hour_of_day', 'day_of_week', 'time_since_signup', 'transactions_last_7d', 'purchase_value_last_7d']\n",
      "E-commerce Categorical Columns for Preprocessor: ['device_id', 'source', 'browser', 'sex']\n",
      "\n",
      "--- Preparing Credit Card Data for Modeling ---\n",
      "Credit Card X shape (before split): (283726, 30)\n",
      "Credit Card Numerical Columns for Preprocessor: ['Time', 'V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10', 'V11', 'V12', 'V13', 'V14', 'V15', 'V16', 'V17', 'V18', 'V19', 'V20', 'V21', 'V22', 'V23', 'V24', 'V25', 'V26', 'V27', 'V28', 'Amount']\n",
      "Credit Card Categorical Columns for Preprocessor: []\n"
     ]
    }
   ],
   "source": [
    "# notebooks/model_building.ipynb - Cell 4: Data Preparation & Column Identification\n",
    "\n",
    "# --- E-commerce Data Preparation ---\n",
    "ecommerce_X = None\n",
    "ecommerce_y = None\n",
    "ecommerce_numerical_cols_for_preprocessor = []\n",
    "ecommerce_categorical_cols_for_preprocessor = []\n",
    "\n",
    "if ecommerce_df is not None:\n",
    "    print(\"\\n--- Preparing E-commerce Data for Modeling ---\")\n",
    "    # Define features (X) and target (y)\n",
    "    # Drop the target column 'class' and other non-feature columns\n",
    "    cols_to_drop_ecommerce = [\n",
    "        'class', 'ip_address', 'lower_bound_ip_address', 'upper_bound_ip_address', 'ip_address_int',\n",
    "        'signup_time', 'purchase_time', 'time_diff_prev_transaction'\n",
    "    ]\n",
    "    ecommerce_X = ecommerce_df.drop(columns=cols_to_drop_ecommerce, errors='ignore')\n",
    "    ecommerce_y = ecommerce_df['class']\n",
    "\n",
    "    # Identify numerical and categorical columns *from this X*\n",
    "    # These lists will be used by the ColumnTransformer in Cell 5\n",
    "    ecommerce_numerical_cols_for_preprocessor = ecommerce_X.select_dtypes(include=np.number).columns.tolist()\n",
    "    ecommerce_categorical_cols_for_preprocessor = ecommerce_X.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "\n",
    "    print(f\"E-commerce X shape (before split): {ecommerce_X.shape}\")\n",
    "    print(f\"E-commerce Numerical Columns for Preprocessor: {ecommerce_numerical_cols_for_preprocessor}\")\n",
    "    print(f\"E-commerce Categorical Columns for Preprocessor: {ecommerce_categorical_cols_for_preprocessor}\")\n",
    "else:\n",
    "    print(\"E-commerce data not loaded. Skipping preparation.\")\n",
    "\n",
    "# --- Credit Card Data Preparation ---\n",
    "creditcard_X = None\n",
    "creditcard_y = None\n",
    "creditcard_numerical_cols_for_preprocessor = []\n",
    "creditcard_categorical_cols_for_preprocessor = [] # Should be empty for this dataset\n",
    "\n",
    "if creditcard_df is not None:\n",
    "    print(\"\\n--- Preparing Credit Card Data for Modeling ---\")\n",
    "    # Credit card data is mostly numerical (V1-V28, Amount, Time)\n",
    "    # 'Class' is the target.\n",
    "    creditcard_X = creditcard_df.drop(columns=['Class'])\n",
    "    creditcard_y = creditcard_df['Class']\n",
    "\n",
    "    # Identify numerical and categorical columns *from this X*\n",
    "    # These lists will be used by the ColumnTransformer in Cell 7\n",
    "    creditcard_numerical_cols_for_preprocessor = creditcard_X.select_dtypes(include=np.number).columns.tolist()\n",
    "    creditcard_categorical_cols_for_preprocessor = creditcard_X.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "\n",
    "    print(f\"Credit Card X shape (before split): {creditcard_X.shape}\")\n",
    "    print(f\"Credit Card Numerical Columns for Preprocessor: {creditcard_numerical_cols_for_preprocessor}\")\n",
    "    print(f\"Credit Card Categorical Columns for Preprocessor: {creditcard_categorical_cols_for_preprocessor}\")\n",
    "else:\n",
    "    print(\"Credit Card data not loaded. Skipping preparation.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b52c950d",
   "metadata": {},
   "source": [
    "# Cell 5: Train-Test Split & Preprocessing (E-commerce)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "987eda6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-26 20:19:07,466 - INFO - Splitting data into training and testing sets (test_size=0.3)...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Splitting E-commerce Data ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-26 20:19:07,548 - INFO - X_train shape: (105778, 18), y_train shape: (105778,)\n",
      "2025-07-26 20:19:07,549 - INFO - X_test shape: (45334, 18), y_test shape: (45334,)\n",
      "2025-07-26 20:19:07,551 - INFO - Training target distribution:\n",
      "class\n",
      "0    0.906351\n",
      "1    0.093649\n",
      "Name: proportion, dtype: float64\n",
      "2025-07-26 20:19:07,553 - INFO - Testing target distribution:\n",
      "class\n",
      "0    0.906362\n",
      "1    0.093638\n",
      "Name: proportion, dtype: float64\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E-commerce Numerical Columns for Preprocessor (after feature selection): ['user_id', 'purchase_value', 'age', 'country', 'hour_of_day', 'day_of_week', 'time_since_signup', 'transactions_last_7d', 'purchase_value_last_7d']\n",
      "E-commerce Categorical Columns for Preprocessor (after feature selection): ['device_id', 'source', 'browser', 'sex']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\skibret\\Downloads\\KAIM\\Week 8\\Project\\fraud_detection_project\\venv8\\Lib\\site-packages\\sklearn\\impute\\_base.py:635: UserWarning: Skipping features without any observed values: ['country']. At least one non-missing value is needed for imputation with strategy='median'.\n",
      "  warnings.warn(\n",
      "c:\\Users\\skibret\\Downloads\\KAIM\\Week 8\\Project\\fraud_detection_project\\venv8\\Lib\\site-packages\\sklearn\\impute\\_base.py:635: UserWarning: Skipping features without any observed values: ['country']. At least one non-missing value is needed for imputation with strategy='median'.\n",
      "  warnings.warn(\n",
      "c:\\Users\\skibret\\Downloads\\KAIM\\Week 8\\Project\\fraud_detection_project\\venv8\\Lib\\site-packages\\sklearn\\impute\\_base.py:635: UserWarning: Skipping features without any observed values: ['country']. At least one non-missing value is needed for imputation with strategy='median'.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E-commerce X_train_processed shape: (105778, 97939)\n",
      "E-commerce X_test_processed shape: (45334, 97939)\n"
     ]
    }
   ],
   "source": [
    "# notebooks/model_building.ipynb - Cell 5: Train-Test Split & Preprocessing (E-commerce)\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline as SklearnPipeline # Alias for sklearn's Pipeline\n",
    "\n",
    "ecommerce_X_train_processed = None\n",
    "ecommerce_X_test_processed = None\n",
    "ecommerce_y_train = None\n",
    "ecommerce_y_test = None\n",
    "\n",
    "if ecommerce_df is not None:\n",
    "    print(\"\\n--- Splitting E-commerce Data ---\")\n",
    "    # Pass the original dataframe and the target column name to split_data\n",
    "    # This function returns X_train, X_test, y_train, y_test, and initial column lists\n",
    "    ecommerce_X_train_raw, ecommerce_X_test_raw, ecommerce_y_train, ecommerce_y_test, \\\n",
    "    initial_ecommerce_categorical_cols, initial_ecommerce_numerical_cols = \\\n",
    "        split_data(ecommerce_df, target_column='class', test_size=0.3, random_state=42)\n",
    "\n",
    "    # Define additional columns to drop from X_train_raw and X_test_raw\n",
    "    # These are columns that were in the original dataframe but should not be features\n",
    "    cols_to_drop_ecommerce_from_X = [\n",
    "        'ip_address', 'lower_bound_ip_address', 'upper_bound_ip_address', 'ip_address_int',\n",
    "        'signup_time', 'purchase_time', 'time_diff_prev_transaction'\n",
    "    ]\n",
    "\n",
    "    # Drop these columns from the X_train and X_test sets\n",
    "    ecommerce_X_train = ecommerce_X_train_raw.drop(columns=cols_to_drop_ecommerce_from_X, errors='ignore')\n",
    "    ecommerce_X_test = ecommerce_X_test_raw.drop(columns=cols_to_drop_ecommerce_from_X, errors='ignore')\n",
    "\n",
    "    # Re-identify numerical and categorical columns *after* dropping the non-feature columns\n",
    "    # This ensures the preprocessor uses the correct column lists\n",
    "    ecommerce_numerical_cols_for_preprocessor = ecommerce_X_train.select_dtypes(include=np.number).columns.tolist()\n",
    "    ecommerce_categorical_cols_for_preprocessor = ecommerce_X_train.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "\n",
    "    print(f\"E-commerce Numerical Columns for Preprocessor (after feature selection): {ecommerce_numerical_cols_for_preprocessor}\")\n",
    "    print(f\"E-commerce Categorical Columns for Preprocessor (after feature selection): {ecommerce_categorical_cols_for_preprocessor}\")\n",
    "\n",
    "    # Create the preprocessing pipeline using ColumnTransformer\n",
    "    # This pipeline will handle imputation, scaling, and one-hot encoding\n",
    "    ecommerce_preprocessor_fitted = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', SklearnPipeline(steps=[\n",
    "                ('imputer', SimpleImputer(strategy='median')), # Impute NaNs in numerical columns\n",
    "                ('scaler', StandardScaler())                    # Scale numerical features\n",
    "            ]), ecommerce_numerical_cols_for_preprocessor),\n",
    "            ('cat', OneHotEncoder(handle_unknown='ignore'), ecommerce_categorical_cols_for_preprocessor) # One-hot encode categorical features\n",
    "        ],\n",
    "        remainder='passthrough' # Keep any other columns not specified (e.g., if you added more later)\n",
    "    ).fit(ecommerce_X_train) # Fit the preprocessor ONLY on the training data to prevent data leakage\n",
    "\n",
    "    # Transform both training and testing sets using the fitted preprocessor\n",
    "    ecommerce_X_train_processed = ecommerce_preprocessor_fitted.transform(ecommerce_X_train)\n",
    "    ecommerce_X_test_processed = ecommerce_preprocessor_fitted.transform(ecommerce_X_test)\n",
    "\n",
    "    print(f\"E-commerce X_train_processed shape: {ecommerce_X_train_processed.shape}\")\n",
    "    print(f\"E-commerce X_test_processed shape: {ecommerce_X_test_processed.shape}\")\n",
    "else:\n",
    "    print(\"E-commerce data (df) not available for splitting and preprocessing.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbda12a1",
   "metadata": {},
   "source": [
    "# Cell 6: Imbalance Handling (E-commerce)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f45f5be3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-26 20:19:08,789 - INFO - Handling class imbalance using strategy: SMOTE...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Handling Imbalance for E-commerce Training Data (SMOTE) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-26 20:19:13,371 - INFO - Resampling complete. Original minority count: 9906, majority count: 95872\n",
      "2025-07-26 20:19:13,372 - INFO - Resampled data shape: (191744, 97939), target shape: (191744,)\n",
      "2025-07-26 20:19:13,378 - INFO - Resampled target distribution:\n",
      "class\n",
      "0    0.5\n",
      "1    0.5\n",
      "Name: proportion, dtype: float64\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E-commerce Resampled X_train shape: (191744, 97939)\n",
      "E-commerce Resampled y_train distribution:\n",
      "class\n",
      "0    0.5\n",
      "1    0.5\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# notebooks/model_building.ipynb - Cell 6: Imbalance Handling (E-commerce)\n",
    "\n",
    "ecommerce_X_train_resampled = None\n",
    "ecommerce_y_train_resampled = None\n",
    "\n",
    "if ecommerce_X_train_processed is not None and ecommerce_y_train is not None:\n",
    "    print(\"\\n--- Handling Imbalance for E-commerce Training Data (SMOTE) ---\")\n",
    "    ecommerce_X_train_resampled, ecommerce_y_train_resampled = \\\n",
    "        handle_imbalance(ecommerce_X_train_processed, ecommerce_y_train, strategy='SMOTE', random_state=42)\n",
    "\n",
    "    print(f\"E-commerce Resampled X_train shape: {ecommerce_X_train_resampled.shape}\")\n",
    "    print(f\"E-commerce Resampled y_train distribution:\\n{ecommerce_y_train_resampled.value_counts(normalize=True)}\")\n",
    "else:\n",
    "    print(\"E-commerce processed training data not available for imbalance handling.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f85afeb5",
   "metadata": {},
   "source": [
    "# E-commerce Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3fdeed3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-26 20:19:13,407 - INFO - Training Logistic Regression model...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Training Models for E-commerce Fraud Detection ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-26 20:19:19,232 - INFO - Logistic Regression training complete.\n",
      "2025-07-26 20:19:19,263 - INFO - Evaluating model performance...\n",
      "2025-07-26 20:19:19,374 - INFO - Accuracy: 0.9459\n",
      "2025-07-26 20:19:19,376 - INFO - Precision: 0.7995\n",
      "2025-07-26 20:19:19,380 - INFO - Recall: 0.5637\n",
      "2025-07-26 20:19:19,383 - INFO - F1-Score: 0.6612\n",
      "2025-07-26 20:19:19,389 - INFO - AUC-ROC: 0.7696\n",
      "2025-07-26 20:19:19,393 - INFO - \n",
      "Confusion Matrix:\n",
      "[[40489   600]\n",
      " [ 1852  2393]]\n",
      "2025-07-26 20:19:19,395 - INFO - \n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.99      0.97     41089\n",
      "           1       0.80      0.56      0.66      4245\n",
      "\n",
      "    accuracy                           0.95     45334\n",
      "   macro avg       0.88      0.77      0.82     45334\n",
      "weighted avg       0.94      0.95      0.94     45334\n",
      "\n",
      "2025-07-26 20:19:19,399 - INFO - Training Decision Tree Classifier model...\n",
      "2025-07-26 20:22:42,268 - INFO - Decision Tree Classifier training complete.\n",
      "2025-07-26 20:22:42,586 - INFO - Evaluating model performance...\n",
      "2025-07-26 20:22:42,675 - INFO - Accuracy: 0.9539\n",
      "2025-07-26 20:22:42,678 - INFO - Precision: 0.9407\n",
      "2025-07-26 20:22:42,680 - INFO - Recall: 0.5420\n",
      "2025-07-26 20:22:42,683 - INFO - F1-Score: 0.6878\n",
      "2025-07-26 20:22:42,684 - INFO - AUC-ROC: 0.7693\n",
      "2025-07-26 20:22:42,698 - INFO - \n",
      "Confusion Matrix:\n",
      "[[40944   145]\n",
      " [ 1944  2301]]\n",
      "2025-07-26 20:22:42,701 - INFO - \n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      1.00      0.98     41089\n",
      "           1       0.94      0.54      0.69      4245\n",
      "\n",
      "    accuracy                           0.95     45334\n",
      "   macro avg       0.95      0.77      0.83     45334\n",
      "weighted avg       0.95      0.95      0.95     45334\n",
      "\n",
      "2025-07-26 20:22:42,703 - INFO - Training Random Forest Classifier model...\n",
      "2025-07-26 20:31:13,657 - INFO - Random Forest Classifier training complete.\n",
      "2025-07-26 20:31:37,537 - INFO - Evaluating model performance...\n",
      "2025-07-26 20:31:37,611 - INFO - Accuracy: 0.9550\n",
      "2025-07-26 20:31:37,613 - INFO - Precision: 0.9674\n",
      "2025-07-26 20:31:37,614 - INFO - Recall: 0.5376\n",
      "2025-07-26 20:31:37,615 - INFO - F1-Score: 0.6911\n",
      "2025-07-26 20:31:37,615 - INFO - AUC-ROC: 0.7776\n",
      "2025-07-26 20:31:37,616 - INFO - \n",
      "Confusion Matrix:\n",
      "[[41012    77]\n",
      " [ 1963  2282]]\n",
      "2025-07-26 20:31:37,617 - INFO - \n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      1.00      0.98     41089\n",
      "           1       0.97      0.54      0.69      4245\n",
      "\n",
      "    accuracy                           0.96     45334\n",
      "   macro avg       0.96      0.77      0.83     45334\n",
      "weighted avg       0.96      0.96      0.95     45334\n",
      "\n",
      "2025-07-26 20:31:37,618 - INFO - Training Gradient Boosting Classifier model...\n",
      "2025-07-26 20:53:58,410 - INFO - Gradient Boosting Classifier training complete.\n",
      "2025-07-26 20:53:58,590 - INFO - Evaluating model performance...\n",
      "2025-07-26 20:53:58,634 - INFO - Accuracy: 0.9523\n",
      "2025-07-26 20:53:58,635 - INFO - Precision: 0.9160\n",
      "2025-07-26 20:53:58,635 - INFO - Recall: 0.5397\n",
      "2025-07-26 20:53:58,636 - INFO - F1-Score: 0.6792\n",
      "2025-07-26 20:53:58,637 - INFO - AUC-ROC: 0.7659\n",
      "2025-07-26 20:53:58,638 - INFO - \n",
      "Confusion Matrix:\n",
      "[[40879   210]\n",
      " [ 1954  2291]]\n",
      "2025-07-26 20:53:58,638 - INFO - \n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.99      0.97     41089\n",
      "           1       0.92      0.54      0.68      4245\n",
      "\n",
      "    accuracy                           0.95     45334\n",
      "   macro avg       0.94      0.77      0.83     45334\n",
      "weighted avg       0.95      0.95      0.95     45334\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- E-commerce Model Training and Evaluation Complete ---\n",
      "\n",
      "Model: Logistic Regression\n",
      "  AUC-ROC: 0.7696\n",
      "  Precision: 0.7995\n",
      "  Recall: 0.5637\n",
      "  F1-Score: 0.6612\n",
      "\n",
      "Model: Decision Tree\n",
      "  AUC-ROC: 0.7693\n",
      "  Precision: 0.9407\n",
      "  Recall: 0.5420\n",
      "  F1-Score: 0.6878\n",
      "\n",
      "Model: Random Forest\n",
      "  AUC-ROC: 0.7776\n",
      "  Precision: 0.9674\n",
      "  Recall: 0.5376\n",
      "  F1-Score: 0.6911\n",
      "\n",
      "Model: Gradient Boosting\n",
      "  AUC-ROC: 0.7659\n",
      "  Precision: 0.9160\n",
      "  Recall: 0.5397\n",
      "  F1-Score: 0.6792\n"
     ]
    }
   ],
   "source": [
    "# notebooks/model_building.ipynb - NEW Cell 7: E-commerce Model Training and Evaluation\n",
    "\n",
    "ecommerce_models = {}\n",
    "ecommerce_results = {}\n",
    "\n",
    "if ecommerce_X_train_resampled is not None and ecommerce_y_train_resampled is not None:\n",
    "    print(\"\\n--- Training Models for E-commerce Fraud Detection ---\")\n",
    "\n",
    "    # --- Logistic Regression ---\n",
    "    lr_model_ecommerce = train_logistic_regression(ecommerce_X_train_resampled, ecommerce_y_train_resampled)\n",
    "    ecommerce_models['Logistic Regression'] = lr_model_ecommerce\n",
    "    y_pred_lr_ecommerce = lr_model_ecommerce.predict(ecommerce_X_test_processed)\n",
    "    y_prob_lr_ecommerce = lr_model_ecommerce.predict_proba(ecommerce_X_test_processed)[:, 1]\n",
    "    ecommerce_results['Logistic Regression'] = evaluate_model(lr_model_ecommerce, ecommerce_X_test_processed, ecommerce_y_test, y_pred_lr_ecommerce, y_prob_lr_ecommerce)\n",
    "\n",
    "    # --- Decision Tree ---\n",
    "    dt_model_ecommerce = train_decision_tree(ecommerce_X_train_resampled, ecommerce_y_train_resampled)\n",
    "    ecommerce_models['Decision Tree'] = dt_model_ecommerce\n",
    "    y_pred_dt_ecommerce = dt_model_ecommerce.predict(ecommerce_X_test_processed)\n",
    "    y_prob_dt_ecommerce = dt_model_ecommerce.predict_proba(ecommerce_X_test_processed)[:, 1]\n",
    "    ecommerce_results['Decision Tree'] = evaluate_model(dt_model_ecommerce, ecommerce_X_test_processed, ecommerce_y_test, y_pred_dt_ecommerce, y_prob_dt_ecommerce)\n",
    "\n",
    "    # --- Random Forest ---\n",
    "    rf_model_ecommerce = train_random_forest(ecommerce_X_train_resampled, ecommerce_y_train_resampled)\n",
    "    ecommerce_models['Random Forest'] = rf_model_ecommerce\n",
    "    y_pred_rf_ecommerce = rf_model_ecommerce.predict(ecommerce_X_test_processed)\n",
    "    y_prob_rf_ecommerce = rf_model_ecommerce.predict_proba(ecommerce_X_test_processed)[:, 1]\n",
    "    ecommerce_results['Random Forest'] = evaluate_model(rf_model_ecommerce, ecommerce_X_test_processed, ecommerce_y_test, y_pred_rf_ecommerce, y_prob_rf_ecommerce)\n",
    "\n",
    "    # --- Gradient Boosting ---\n",
    "    gb_model_ecommerce = train_gradient_boosting(ecommerce_X_train_resampled, ecommerce_y_train_resampled)\n",
    "    ecommerce_models['Gradient Boosting'] = gb_model_ecommerce\n",
    "    y_pred_gb_ecommerce = gb_model_ecommerce.predict(ecommerce_X_test_processed)\n",
    "    y_prob_gb_ecommerce = gb_model_ecommerce.predict_proba(ecommerce_X_test_processed)[:, 1]\n",
    "    ecommerce_results['Gradient Boosting'] = evaluate_model(gb_model_ecommerce, ecommerce_X_test_processed, ecommerce_y_test, y_pred_gb_ecommerce, y_prob_gb_ecommerce)\n",
    "\n",
    "   \n",
    "    print(\"\\n--- E-commerce Model Training and Evaluation Complete ---\")\n",
    "    for model_name, metrics in ecommerce_results.items():\n",
    "        print(f\"\\nModel: {model_name}\")\n",
    "        print(f\"  AUC-ROC: {metrics['roc_auc']:.4f}\")\n",
    "        print(f\"  Precision: {metrics['precision']:.4f}\")\n",
    "        print(f\"  Recall: {metrics['recall']:.4f}\")\n",
    "        print(f\"  F1-Score: {metrics['f1_score']:.4f}\")\n",
    "else:\n",
    "    print(\"E-commerce resampled training data not available for model training.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7c65378",
   "metadata": {},
   "source": [
    "# Cell 7: Train-Test Split & Preprocessing (Credit Card)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ca39701f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-26 20:53:58,783 - INFO - Splitting data into training and testing sets (test_size=0.3)...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Splitting Credit Card Data ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-26 20:53:59,045 - INFO - X_train shape: (198608, 30), y_train shape: (198608,)\n",
      "2025-07-26 20:53:59,046 - INFO - X_test shape: (85118, 30), y_test shape: (85118,)\n",
      "2025-07-26 20:53:59,048 - INFO - Training target distribution:\n",
      "Class\n",
      "0    0.998333\n",
      "1    0.001667\n",
      "Name: proportion, dtype: float64\n",
      "2025-07-26 20:53:59,050 - INFO - Testing target distribution:\n",
      "Class\n",
      "0    0.998332\n",
      "1    0.001668\n",
      "Name: proportion, dtype: float64\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Credit Card Numerical Columns for Preprocessor (after feature selection): ['Time', 'V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10', 'V11', 'V12', 'V13', 'V14', 'V15', 'V16', 'V17', 'V18', 'V19', 'V20', 'V21', 'V22', 'V23', 'V24', 'V25', 'V26', 'V27', 'V28', 'Amount']\n",
      "Credit Card Categorical Columns for Preprocessor (after feature selection): []\n",
      "Credit Card X_train_processed shape: (198608, 30)\n",
      "Credit Card X_test_processed shape: (85118, 30)\n"
     ]
    }
   ],
   "source": [
    "# notebooks/model_building.ipynb - Cell 7: Train-Test Split & Preprocessing (Credit Card)\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder # Re-import if this cell is run independently\n",
    "from sklearn.impute import SimpleImputer # Re-import if this cell is run independently\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline as SklearnPipeline # Alias for sklearn's Pipeline\n",
    "\n",
    "creditcard_X_train_processed = None\n",
    "creditcard_X_test_processed = None\n",
    "creditcard_y_train = None\n",
    "creditcard_y_test = None\n",
    "\n",
    "if creditcard_df is not None:\n",
    "    print(\"\\n--- Splitting Credit Card Data ---\")\n",
    "    # Pass the original dataframe and the target column name to split_data\n",
    "    creditcard_X_train_raw, creditcard_X_test_raw, creditcard_y_train, creditcard_y_test, \\\n",
    "    initial_creditcard_categorical_cols, initial_creditcard_numerical_cols = \\\n",
    "        split_data(creditcard_df, target_column='Class', test_size=0.3, random_state=42)\n",
    "\n",
    "    # For credit card data, typically no extra columns to drop from X_train/X_test\n",
    "    # as V-features, Time, Amount are all relevant.\n",
    "    creditcard_X_train = creditcard_X_train_raw.copy()\n",
    "    creditcard_X_test = creditcard_X_test_raw.copy()\n",
    "\n",
    "    # Re-identify numerical and categorical columns (should be mostly numerical)\n",
    "    creditcard_numerical_cols_for_preprocessor = creditcard_X_train.select_dtypes(include=np.number).columns.tolist()\n",
    "    creditcard_categorical_cols_for_preprocessor = creditcard_X_train.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "\n",
    "    print(f\"Credit Card Numerical Columns for Preprocessor (after feature selection): {creditcard_numerical_cols_for_preprocessor}\")\n",
    "    print(f\"Credit Card Categorical Columns for Preprocessor (after feature selection): {creditcard_categorical_cols_for_preprocessor}\")\n",
    "\n",
    "    # Create the preprocessing pipeline using ColumnTransformer\n",
    "    # Only numerical features are expected for credit card data, so only a numerical pipeline\n",
    "    creditcard_preprocessor_fitted = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', SklearnPipeline(steps=[\n",
    "                ('imputer', SimpleImputer(strategy='median')), # Impute NaNs in numerical columns\n",
    "                ('scaler', StandardScaler())                    # Scale numerical features\n",
    "            ]), creditcard_numerical_cols_for_preprocessor)\n",
    "            # No 'cat' transformer needed if no categorical columns are present\n",
    "        ],\n",
    "        remainder='passthrough'\n",
    "    ).fit(creditcard_X_train) # Fit the preprocessor ONLY on the training data\n",
    "\n",
    "    # Transform both training and testing sets\n",
    "    creditcard_X_train_processed = creditcard_preprocessor_fitted.transform(creditcard_X_train)\n",
    "    creditcard_X_test_processed = creditcard_preprocessor_fitted.transform(creditcard_X_test)\n",
    "\n",
    "    print(f\"Credit Card X_train_processed shape: {creditcard_X_train_processed.shape}\")\n",
    "    print(f\"Credit Card X_test_processed shape: {creditcard_X_test_processed.shape}\")\n",
    "else:\n",
    "    print(\"Credit Card data (df) not available for splitting and preprocessing.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79cb955b",
   "metadata": {},
   "source": [
    "# Cell 8: Imbalance Handling (Credit Card)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4fb5c695",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-26 20:54:00,146 - INFO - Handling class imbalance using strategy: SMOTE_and_Undersample...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Handling Imbalance for Credit Card Training Data (SMOTE_and_Undersample) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-26 20:54:03,264 - INFO - Resampling complete. Original minority count: 331, majority count: 198277\n",
      "2025-07-26 20:54:03,265 - INFO - Resampled data shape: (59481, 30), target shape: (59481,)\n",
      "2025-07-26 20:54:03,266 - INFO - Resampled target distribution:\n",
      "Class\n",
      "0    0.666667\n",
      "1    0.333333\n",
      "Name: proportion, dtype: float64\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Credit Card Resampled X_train shape: (59481, 30)\n",
      "Credit Card Resampled y_train distribution:\n",
      "Class\n",
      "0    0.666667\n",
      "1    0.333333\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# notebooks/model_building.ipynb - Cell 8: Imbalance Handling (Credit Card)\n",
    "\n",
    "creditcard_X_train_resampled = None\n",
    "creditcard_y_train_resampled = None\n",
    "\n",
    "if creditcard_X_train_processed is not None and creditcard_y_train is not None:\n",
    "    print(\"\\n--- Handling Imbalance for Credit Card Training Data (SMOTE_and_Undersample) ---\")\n",
    "    # For credit card data, SMOTE_and_Undersample might be more effective due to extreme imbalance\n",
    "    creditcard_X_train_resampled, creditcard_y_train_resampled = \\\n",
    "        handle_imbalance(creditcard_X_train_processed, creditcard_y_train, strategy='SMOTE_and_Undersample', random_state=42)\n",
    "\n",
    "    print(f\"Credit Card Resampled X_train shape: {creditcard_X_train_resampled.shape}\")\n",
    "    print(f\"Credit Card Resampled y_train distribution:\\n{creditcard_y_train_resampled.value_counts(normalize=True)}\")\n",
    "else:\n",
    "    print(\"Credit Card processed training data not available for imbalance handling.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac2ec63e",
   "metadata": {},
   "source": [
    "# Credit Card Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cb1e9ab8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-26 20:54:03,276 - INFO - Training Logistic Regression model...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Training Models for Credit Card Fraud Detection ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-26 20:54:03,965 - INFO - Logistic Regression training complete.\n",
      "2025-07-26 20:54:03,980 - INFO - Evaluating model performance...\n",
      "2025-07-26 20:54:04,078 - INFO - Accuracy: 0.9870\n",
      "2025-07-26 20:54:04,079 - INFO - Precision: 0.1035\n",
      "2025-07-26 20:54:04,080 - INFO - Recall: 0.8873\n",
      "2025-07-26 20:54:04,080 - INFO - F1-Score: 0.1854\n",
      "2025-07-26 20:54:04,081 - INFO - AUC-ROC: 0.9683\n",
      "2025-07-26 20:54:04,082 - INFO - \n",
      "Confusion Matrix:\n",
      "[[83885  1091]\n",
      " [   16   126]]\n",
      "2025-07-26 20:54:04,082 - INFO - \n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.99      0.99     84976\n",
      "           1       0.10      0.89      0.19       142\n",
      "\n",
      "    accuracy                           0.99     85118\n",
      "   macro avg       0.55      0.94      0.59     85118\n",
      "weighted avg       1.00      0.99      0.99     85118\n",
      "\n",
      "2025-07-26 20:54:04,084 - INFO - Training Decision Tree Classifier model...\n",
      "2025-07-26 20:54:08,158 - INFO - Decision Tree Classifier training complete.\n",
      "2025-07-26 20:54:08,178 - INFO - Evaluating model performance...\n",
      "2025-07-26 20:54:08,227 - INFO - Accuracy: 0.9926\n",
      "2025-07-26 20:54:08,228 - INFO - Precision: 0.1540\n",
      "2025-07-26 20:54:08,228 - INFO - Recall: 0.7676\n",
      "2025-07-26 20:54:08,229 - INFO - F1-Score: 0.2565\n",
      "2025-07-26 20:54:08,230 - INFO - AUC-ROC: 0.8803\n",
      "2025-07-26 20:54:08,230 - INFO - \n",
      "Confusion Matrix:\n",
      "[[84377   599]\n",
      " [   33   109]]\n",
      "2025-07-26 20:54:08,231 - INFO - \n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.99      1.00     84976\n",
      "           1       0.15      0.77      0.26       142\n",
      "\n",
      "    accuracy                           0.99     85118\n",
      "   macro avg       0.58      0.88      0.63     85118\n",
      "weighted avg       1.00      0.99      1.00     85118\n",
      "\n",
      "2025-07-26 20:54:08,232 - INFO - Training Random Forest Classifier model...\n",
      "2025-07-26 20:54:23,442 - INFO - Random Forest Classifier training complete.\n",
      "2025-07-26 20:54:23,883 - INFO - Evaluating model performance...\n",
      "2025-07-26 20:54:23,993 - INFO - Accuracy: 0.9993\n",
      "2025-07-26 20:54:23,994 - INFO - Precision: 0.7651\n",
      "2025-07-26 20:54:23,994 - INFO - Recall: 0.8028\n",
      "2025-07-26 20:54:23,995 - INFO - F1-Score: 0.7835\n",
      "2025-07-26 20:54:23,996 - INFO - AUC-ROC: 0.9774\n",
      "2025-07-26 20:54:23,996 - INFO - \n",
      "Confusion Matrix:\n",
      "[[84941    35]\n",
      " [   28   114]]\n",
      "2025-07-26 20:54:23,997 - INFO - \n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     84976\n",
      "           1       0.77      0.80      0.78       142\n",
      "\n",
      "    accuracy                           1.00     85118\n",
      "   macro avg       0.88      0.90      0.89     85118\n",
      "weighted avg       1.00      1.00      1.00     85118\n",
      "\n",
      "2025-07-26 20:54:23,998 - INFO - Training Gradient Boosting Classifier model...\n",
      "2025-07-26 20:56:11,714 - INFO - Gradient Boosting Classifier training complete.\n",
      "2025-07-26 20:56:12,034 - INFO - Evaluating model performance...\n",
      "2025-07-26 20:56:12,112 - INFO - Accuracy: 0.9937\n",
      "2025-07-26 20:56:12,113 - INFO - Precision: 0.1909\n",
      "2025-07-26 20:56:12,114 - INFO - Recall: 0.8592\n",
      "2025-07-26 20:56:12,114 - INFO - F1-Score: 0.3124\n",
      "2025-07-26 20:56:12,115 - INFO - AUC-ROC: 0.9670\n",
      "2025-07-26 20:56:12,116 - INFO - \n",
      "Confusion Matrix:\n",
      "[[84459   517]\n",
      " [   20   122]]\n",
      "2025-07-26 20:56:12,116 - INFO - \n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.99      1.00     84976\n",
      "           1       0.19      0.86      0.31       142\n",
      "\n",
      "    accuracy                           0.99     85118\n",
      "   macro avg       0.60      0.93      0.65     85118\n",
      "weighted avg       1.00      0.99      1.00     85118\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Credit Card Model Training and Evaluation Complete ---\n",
      "\n",
      "Model: Logistic Regression\n",
      "  AUC-ROC: 0.9683\n",
      "  Precision: 0.1035\n",
      "  Recall: 0.8873\n",
      "  F1-Score: 0.1854\n",
      "\n",
      "Model: Decision Tree\n",
      "  AUC-ROC: 0.8803\n",
      "  Precision: 0.1540\n",
      "  Recall: 0.7676\n",
      "  F1-Score: 0.2565\n",
      "\n",
      "Model: Random Forest\n",
      "  AUC-ROC: 0.9774\n",
      "  Precision: 0.7651\n",
      "  Recall: 0.8028\n",
      "  F1-Score: 0.7835\n",
      "\n",
      "Model: Gradient Boosting\n",
      "  AUC-ROC: 0.9670\n",
      "  Precision: 0.1909\n",
      "  Recall: 0.8592\n",
      "  F1-Score: 0.3124\n"
     ]
    }
   ],
   "source": [
    "# notebooks/model_building.ipynb - NEW Cell 9: Credit Card Model Training and Evaluation\n",
    "\n",
    "creditcard_models = {}\n",
    "creditcard_results = {}\n",
    "\n",
    "if creditcard_X_train_resampled is not None and creditcard_y_train_resampled is not None:\n",
    "    print(\"\\n--- Training Models for Credit Card Fraud Detection ---\")\n",
    "\n",
    "    # --- Logistic Regression ---\n",
    "    lr_model_creditcard = train_logistic_regression(creditcard_X_train_resampled, creditcard_y_train_resampled)\n",
    "    creditcard_models['Logistic Regression'] = lr_model_creditcard\n",
    "    y_pred_lr_creditcard = lr_model_creditcard.predict(creditcard_X_test_processed)\n",
    "    y_prob_lr_creditcard = lr_model_creditcard.predict_proba(creditcard_X_test_processed)[:, 1]\n",
    "    creditcard_results['Logistic Regression'] = evaluate_model(lr_model_creditcard, creditcard_X_test_processed, creditcard_y_test, y_pred_lr_creditcard, y_prob_lr_creditcard)\n",
    "\n",
    "    # --- Decision Tree ---\n",
    "    dt_model_creditcard = train_decision_tree(creditcard_X_train_resampled, creditcard_y_train_resampled)\n",
    "    creditcard_models['Decision Tree'] = dt_model_creditcard\n",
    "    y_pred_dt_creditcard = dt_model_creditcard.predict(creditcard_X_test_processed)\n",
    "    y_prob_dt_creditcard = dt_model_creditcard.predict_proba(creditcard_X_test_processed)[:, 1]\n",
    "    creditcard_results['Decision Tree'] = evaluate_model(dt_model_creditcard, creditcard_X_test_processed, creditcard_y_test, y_pred_dt_creditcard, y_prob_dt_creditcard)\n",
    "\n",
    "    # --- Random Forest ---\n",
    "    rf_model_creditcard = train_random_forest(creditcard_X_train_resampled, creditcard_y_train_resampled)\n",
    "    creditcard_models['Random Forest'] = rf_model_creditcard\n",
    "    y_pred_rf_creditcard = rf_model_creditcard.predict(creditcard_X_test_processed)\n",
    "    y_prob_rf_creditcard = rf_model_creditcard.predict_proba(creditcard_X_test_processed)[:, 1]\n",
    "    creditcard_results['Random Forest'] = evaluate_model(rf_model_creditcard, creditcard_X_test_processed, creditcard_y_test, y_pred_rf_creditcard, y_prob_rf_creditcard)\n",
    "\n",
    "    # --- Gradient Boosting ---\n",
    "    gb_model_creditcard = train_gradient_boosting(creditcard_X_train_resampled, creditcard_y_train_resampled)\n",
    "    creditcard_models['Gradient Boosting'] = gb_model_creditcard\n",
    "    y_pred_gb_creditcard = gb_model_creditcard.predict(creditcard_X_test_processed)\n",
    "    y_prob_gb_creditcard = gb_model_creditcard.predict_proba(creditcard_X_test_processed)[:, 1]\n",
    "    creditcard_results['Gradient Boosting'] = evaluate_model(gb_model_creditcard, creditcard_X_test_processed, creditcard_y_test, y_pred_gb_creditcard, y_prob_gb_creditcard)\n",
    "\n",
    "   \n",
    "    print(\"\\n--- Credit Card Model Training and Evaluation Complete ---\")\n",
    "    for model_name, metrics in creditcard_results.items():\n",
    "        print(f\"\\nModel: {model_name}\")\n",
    "        print(f\"  AUC-ROC: {metrics['roc_auc']:.4f}\")\n",
    "        print(f\"  Precision: {metrics['precision']:.4f}\")\n",
    "        print(f\"  Recall: {metrics['recall']:.4f}\")\n",
    "        print(f\"  F1-Score: {metrics['f1_score']:.4f}\")\n",
    "else:\n",
    "    print(\"Credit Card resampled training data not available for model training.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv8",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
